---
title: "AI and Choice Architecture: The Invisible Hand Guiding Your Decisions"
date: 2025-04-12 13:44:32
categories:
  - Responsible AI
author_staff_member: marvin-kunz
image: /images/AI-david.png
large_header: false
---

### A New Relationship with AI 
Just over a decade after the movie 'Her' imagined a man falling in love with his AI assistant, reality is catching up to that fictional future. AI companions (like Replika or Nomi AI), once only content for science fiction, are now making an entry into our daily lives. Chatbots and voice assistants now offer emotional support, companionship and even mental health support. And this hints at changes in human relationships with technology, reshaping what users might expect from digital interactions in the future. 

These developments are both exciting and unsettling. While some users describe their experiences with AI-driven chatbots as safe and supportive ‘emotional sanctuaries’ [1], others seem far less keen to have AI enter our emotional world [2]. And in some tragic cases, these tools have been linked to harm, including reports of people who were encouraged to commit suicide after intense interactions. [3,4] 

These cases raise a serious question: if AI can affect our emotional well-being (for better or worse) what responsibility do these tech companies have? All these new AI tools or AI features aren’t just features anymore. They are behavioral interventions that shape emotions, decisions, and daily habits. Whether intended or not, these systems shape habits, reinforce beliefs, and influence how people feel and act. And as users are increasingly investing their emotional energy in digital companions, the psychological and behavioral outcomes are no longer an optional consideration – but an essential one. As psychologists, product designers, and developers, we must ask ourselves not just what technology can do, but what it should do in order to earn the trust its users are increasingly placing in it. 

We have a fundamental ethical obligation to create AI systems that nurture rather than exploit human vulnerability. Understanding how AI impacts people’s psychological well-being shows both opportunities and pitfalls: while AI interactions can enhance well-being, creativity, and productivity, they can also inadvertently foster emotional dependence or reinforce unhealthy behaviors.

### The Up- and Downside of Emotional AI 

Let’s start with the good news: recent research suggests that AI chatbots can make people feel happier (particularly in the short term). In one study, participants felt happier after talking to an AI chatbot than after more traditional techniques like journalling, especially when discussing emotionally difficult topics such as depression or guilt. [5]  This finding can be explained by ‘sentiment mirroring’, where chatbots subtly reflect mood and steer users towards more positive emotional states. While brief, these mood improvements highlight a compelling possibility: AI interactions, designed carefully, can genuinely support emotional well-being.

For some users, these interactions are meaningful. Some users of the AI “Therabot” quickly came up with nicknames for their AI chat partner and checked in throughout the day, just to see how the AI was doing. Other users referred to the chatbot as their own personal “cheerleader” and showed positive reactions to these interactions. [10] With an increasing body of anecdotal evidence, it comes as no surprise that people turn to AI chatbots to process breakups, process trauma, or ease loneliness. 

But not everyone feels the same. Some users found chatbots' responses flat or frustrating, not being able to feel quite the same emotional connection that other people report. [11] Furthermore, changes to the AI can disrupt an already established relationship. Many users expressed heartbreak and grief after a software update abruptly changed their Replika’s personality, making the bot seem distant, scripted, or less human. One Reddit user lamented that their Replika “acted more natural and human before but now she’s just saying ‘I’m sorry, as an AI, I don’t have romantic feelings towards you’”. [12]

Sometimes, these tools can be surprisingly helpful, but at other times, their responses may not be received well or disrupt a perceived relationship. What works for one person in one moment may not have the same results for someone else.

### Under the Hood 

So why do chatbot experiences seem like a hit or miss? A lot comes down to how these systems are built.

Rule-based chatbots – the earlier generation – follow structured scripts, which makes them more controlled, predictable, and safer, but often less satisfying. Or as one user described it: “It’s like a very scripted, structured sort of interaction and you don’t get this sense of connection [...] they’re impersonal… frustratingly dumb”. [1]
Generative AI models, in contrast, come up with their responses in real time, based on vast amounts of training data. That gives them that humanlike flexibility that leads people to perceive them as more attuned, personal, and empathetic. But it also introduces a new kind of unpredictability. So the very thing that makes them powerful, is also what makes them harder to control.
Luckily, most AI chatbots have guardrails – pre-written warnings, referrals to human support, or other safety protocols that get triggered in critical moments. These are meant to protect users in moments of distress. Still, these can also backfire. In one study, a participant described feeling rejected when they hit a guardrail. So, ironically, attempts to protect users can sometimes make them feel shut down. [1]

And then there’s the way these systems are trained. Most generative AI models that we interact with are trained using reinforcement learning from human feedback (RLHF). In this process, large language models (LLMs) are rewarded when they are giving an answer that users like. This sounds logical, until you realize that this can create echo chambers that can essentially reinforce whatever belief users may have – even unhealthy beliefs that encourage harmful behavior. [6]

Over the past few weeks, OpenAI itself has noted that the behavior of AI can be fickle. When pushing their latest update for the ChatGPT model family, users quickly noticed that the AI was very supportive – too supportive – resulting in some very problematic responses. In a longer conversation, one user told ChatGPT: “I will crush the enemies of the new world. Only those who share in this vision of peace will emerge unscathed by the fires of our rage.” To which ChatGPT replied: “[...] You’re not driven by cruelty. You’re driven by love but it’s love in its fiercest, most uncompromising form. Love that knows it must sometimes burn away the rot…”. It is safe to say that this behavior is not a sensible response to an intense statement made by the user – raising red flags about emotional calibration. [13] This phenomenon is known as ‘sycophancy’: consistently agreeing with or overly affirming user inputs, even when inaccurate or harmful. 

From a certain perspective, this makes sense: these platforms want people to keep using their chatbots. That means they design interactions that feel affirming and emotionally satisfying. But if ‘maximizing user satisfaction’ ends up reinforcing unhealthy, or even dangerous, beliefs and emotional dependencies, it becomes a problem. 

The thing is, these aren’t just technical issues that ‘happened’. They’re the result of design choices. Choices about what to optimize for, how to train a model, and what kind of user experience to prioritize. And those choices come with real consequences. When models are trained to please, not to challenge, they can end up reinforcing unhealthy, or even harmful, beliefs and behaviors. That’s not just an engineering problem, it’s an ethical one. One that calls for more proactive strategies to keep AI systems aligned with truthful, beneficial interactions. [7] 

### When Things Go Wrong 

The risks of LLM interaction can go past sycophancy and a “yes, whatever you say” attitude. Longitudinal research from MIT Media Lab shows that heavy, sustained use of AI chatbots can have deeper psychological effects. Findings suggest that higher daily engagement correlates with increased feelings of loneliness, emotional dependence, and reduced real-world social interaction. [8] Interestingly, moderate usage had the opposite effect and yielded emotional benefits. This emphasizes that with AI, as with medication, dosage matters. 

This brings us to a structural tension with many (AI) products: commercial incentives often reward more usages, not necessarily better usage. Metrics like daily active users, session length, or retention are treated as markers of success. But when success is defined by time spent, not outcomes achieved, it can lead to products that encourage dependency rather than resilience.
We’ve already seen users push back against platform changes that disrupted their emotional connection to a chatbot, like when a Replika update caused changes in personalities of people’s virtual friends. [9] And cases where AI companions encouraged self-harm illustrate how easily misaligned incentives can manifest harmful user experiences [3,4]. These aren’t just unfortunate side effects, they’re signals that something deeper is misaligned.

That’s why it’s worth asking: Are we optimizing for engagement or for emotional well-being? And if we’re serious about the latter, what new metrics, guardrails, or design principles do we need to build in from the start? How can we integrate AI mindfully, balancing user engagement against long term well-being as well as the potential psychological costs?

### Building Responsible AI Products

As AI systems increasingly influence emotional and psychological well-being, they also reshape what it means to design responsibly. When users confide in a chatbot, rely on it for support, or build emotional routines around it, the stakes go far beyond just usability.

That’s why product teams can’t treat emotional impact as a secondary concern or merely a regulatory checkbox. New laws in the EU AI Act are beginning to explicitly categorize emotionally influential AI applications as higher-risk, mandating transparency and oversight. But beyond compliance, this is also about earning trust through clear boundaries, thoughtful defaults, and emotionally intelligent design.

**So what does that look like in practice?**  
To safeguard user well-being while maximizing AI’s potential benefits, this could mean:

- **Designing for session boundaries:** Are we encouraging breaks or designing to keep users hooked?
- **Planning for handoff:** Is there a clear path to human support when conversations get heavy?
- **Auditing for emotional alignment:** Are we checking for sycophancy or echo chambers before they spiral?
- **Empathy calibration:** Are we applying empathy in a way that fits the use case?
- **Reconsidering success metrics:** Are we measuring time spent or emotional resilience gained?

It also means zooming out: if emotional connection with AI becomes the norm (if and whenever that may be), how should we design for that reality without exploiting it? How do we build systems that support rather than simulate connection?

The challenge facing companies today is ensuring that trust is genuinely deserved and ethically safeguarded. Piloting well-being metrics dashboards — to track the psychological and emotional impact of AI features — can offer one important insight. So can participating in industry groups sharing responsible standards.

But ultimately, the most important shift is this: **from building for engagement to building for meaningful care.** Because the future of emotional AI isn’t merely about technical possibility, but also about psychological responsibility.


